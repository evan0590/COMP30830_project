{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Allows plots to appear directly in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import sqlalchemy as sqla\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import csv\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI='database-comp30830.c2kwpm1jk01q.us-east-1.rds.amazonaws.com'\n",
    "PORT='3306'\n",
    "DB='comp30830_db'\n",
    "PASSWORD='Simple12'\n",
    "USER='admin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"mysql+mysqldb://{}:{}@{}:{}/{}\".format(USER, PASSWORD,\n",
    "                                                              URI, PORT, DB), echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bikes=pd.read_sql_table('live_bike_data', engine)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe of this table\n",
    "bikes.to_csv('allBikes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into a dataframe.\n",
    "bikes = pd.read_csv('allBikes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather=pd.read_sql_table('live_weather_data', engine)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe of this table\n",
    "weather.to_csv('allWeather.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into a dataframe.\n",
    "weather = pd.read_csv('allWeather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes['datetime'] = pd.to_datetime(bikes['date'] + ' ' + bikes['time'])\n",
    "weather['datetime'] = pd.to_datetime(weather['date'] + ' ' + weather['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = bikes.sort_values(by='datetime')\n",
    "weather = weather.sort_values(by='datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This can be used as a checkpoint, start from here if you want to run again without having to call from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge_asof(bikes, weather, left_on=\"datetime\", right_on=\"datetime\",direction=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constrict the dataframe to only those times in which the service is available to users.\n",
    "full_df = full_df.drop(full_df[(full_df.datetime.dt.hour > 0) & (full_df.datetime.dt.hour < 5)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create four flags each representing the stage of the day.\n",
    "morning_start = pd.to_datetime(\"05:00:00\").time()\n",
    "morning_end = pd.to_datetime(\"12:00:00\").time()\n",
    "afternoon_start = pd.to_datetime(\"12:01:00\").time()\n",
    "afternoon_end = pd.to_datetime(\"16:59:00\").time()\n",
    "evening_start = pd.to_datetime(\"17:00:00\").time()\n",
    "evening_end = pd.to_datetime(\"20:00:00\").time()\n",
    "night_start = pd.to_datetime(\"20:01:00\").time()\n",
    "night_end = pd.to_datetime(\"23:59:59\").time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['morning'] = np.where((full_df['datetime'].dt.time > morning_start)\n",
    "                         & (full_df['datetime'].dt.time < morning_end),\n",
    "                         1, 0)\n",
    "\n",
    "full_df['afternoon'] = np.where((full_df['datetime'].dt.time > afternoon_start)\n",
    "                         & (full_df['datetime'].dt.time < afternoon_end),\n",
    "                         1, 0)\n",
    "\n",
    "full_df['evening'] = np.where((full_df['datetime'].dt.time > evening_start)\n",
    "                         & (full_df['datetime'].dt.time < evening_end),\n",
    "                         1, 0)\n",
    "\n",
    "full_df['night'] = np.where((full_df['datetime'].dt.time > night_start)\n",
    "                         & (full_df['datetime'].dt.time < night_end),\n",
    "                         1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace days with numbers\n",
    "full_df[\"day_x\"].replace(['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], [0,1,2,3,4,5,6], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a time of day column, based on the hours of the day.\n",
    "full_df['tod'] = full_df.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the usage on a given weekday at a particular station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_test_df = full_df.loc[(full_df['ID'] == 84) & (full_df['day_x'] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, plot the observed data\n",
    "weekday_test_df.plot(kind='scatter', x='tod', y='availableBikes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_test_df = full_df.loc[(full_df['ID'] == 84) & (full_df['day_x'] == 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_test_df.plot(kind='scatter', x='tod', y='availableBikes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the usage on a given weekend day at the same station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_test_df = full_df.loc[(full_df['ID'] == 84) & (full_df['day_x'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot the observed data\n",
    "weekend_test_df.plot(kind='scatter', x='tod', y='availableBikes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From each of these plots, we can observe a variance between usage during the week and during the weekend. Weekdays generally see heavier usage during the morning time, while weekends see more usage during the afternoon/evening time. Therefore, two spearate models will be developed- one for Monday to Friday and another for Saturdays and Sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clouds\n",
    "full_df[\"number\"].replace([801,802,803,804], 'clouds', inplace=True)\n",
    "\n",
    "#clear\n",
    "full_df[\"number\"].replace([800], 'clear', inplace=True)\n",
    "\n",
    "#Atmosphere\n",
    "full_df[\"number\"].replace([701,711,721,731,741,751,761,762,771,781], 'Atmosphere', inplace=True)\n",
    "\n",
    "#snow\n",
    "full_df[\"number\"].replace([600,601,602,611,612,613,615,616,620,621,622], 'snow', inplace=True)\n",
    "\n",
    "#rain\n",
    "full_df[\"number\"].replace([500,501,502,503,504,511,520,521,522,531], 'rainfall', inplace=True)\n",
    "\n",
    "#drizzle\n",
    "full_df[\"number\"].replace([300,301,302,310,311,312,313,314,321], 'drizzle', inplace=True)\n",
    "\n",
    "#thunderstorm\n",
    "full_df[\"number\"].replace([200,201,202,210,211,212,221,230,231,232], 'thunderstorm', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.drop([\"date_x\",\"time_x\",\"status\", \"epoch\", \"main\",\n",
    "         \"description\",\"icon\", \"tempMin\", \"tempMax\", \"tempFeels\", \"humidity\",\n",
    "         \"pressure\", \"windSpeed\",\"windDeg\",\"sunrise\", \"sunset\",\n",
    "             \"date_y\",\"time_y\", \"day_y\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a flag that indicates whether a day is dry (has zero rain)\n",
    "full_df['dry_day'] = (full_df['rain'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = int(input(\"Please enter a number to predict for either availableBikes - (0) or availableBikeStands - (1): \"))\n",
    "if choice == 0:\n",
    "    full_df = full_df.rename(columns={\"availableBikes\": \"target\"})\n",
    "    full_df.drop([\"availableBikeStands\"], axis=1,inplace=True)    \n",
    "else:\n",
    "    full_df = full_df.rename(columns={\"availableBikeStands\": \"target\"})\n",
    "    full_df.drop([\"availableBikes\"], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df = full_df.loc[(full_df['day_x'] >= 0) & (full_df['day_x'] <= 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekend_df = full_df.loc[(full_df['day_x'] >= 5) & (full_df['day_x'] <= 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = int(input(\"Please enter station ID: \"))\n",
    "week_or_weekend = int(input(\"Please choose to predict for week - (0) or weekend - (1): \"))\n",
    "# bikes_or_stands = int(input(\"Please choose to predict either availableBikeStands(0) or availableBikes(1): \"))\n",
    "# Constrain df to a single station on a single day\n",
    "if week_or_weekend == 0:\n",
    "    new_df = week_df.loc[(week_df.ID == station)]\n",
    "else:\n",
    "    new_df = weekend_df.loc[(weekend_df.ID == station)]\n",
    "\n",
    "# Constrain df to a single station on a single day\n",
    "# new_df = full_df.loc[(full_df['ID'] == station) & (full_df['day_x'] == day)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping all columns not necessary for predictive model.\n",
    "new_df.drop([\"ID\", \"datetime\"], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe of this station\n",
    "new_df.to_csv('comp303830_model_multipleLinearRegression.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file into a dataframe.\n",
    "df = pd.read_csv('comp303830_model_multipleLinearRegression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average target(availableBikes/availableBikeStands) in our dataset.\n",
    "# We could use this as a very simple baseline prediction model.\n",
    "# A better prediction model should at least improve on this baseline model.\n",
    "round(df.target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the data:\n",
    "- Trying to find correlations between continuous data and the target feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot the observed data\n",
    "df.plot(kind='scatter', x='rain', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, plot the observed data\n",
    "df.plot(kind='scatter', x='temp', y='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There does not appear to be a clear correlation between the target feature and the continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keep this pandas series for later.\n",
    "## Will be used below.\n",
    "tod_placeholder = df[['tod']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with continuous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace days with numbers\n",
    "df[\"day_x\"].replace([0,1,2,3,4,5,6], ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also do this directly for all categorical features\n",
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features must exclude the target feature\n",
    "column_names = list(df.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[column_names]\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_first = True removes multi-collinearity\n",
    "add_var = pd.get_dummies(X['tod'], prefix='tod', drop_first=True)\n",
    "# Add all the columns to the model data\n",
    "X = X.join(add_var)\n",
    "# Drop the original column that was expanded\n",
    "X.drop(columns=['tod'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model based upon multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with null values\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "df['deg_1_pred'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used repeatedly to compute all metrics\n",
    "def printMetrics(testActualVal, predictions):\n",
    "    #classification evaluation measures\n",
    "    print('\\n==============================================================================')\n",
    "    print(\"MAE: \", metrics.mean_absolute_error(testActualVal, predictions))\n",
    "    #print(\"MSE: \", metrics.mean_squared_error(testActualVal, predictions))\n",
    "    print(\"RMSE: \", metrics.mean_squared_error(testActualVal, predictions)**0.5)\n",
    "    print(\"R2: \", metrics.r2_score(testActualVal, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printMetrics(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['target', 'deg_1_pred']].plot(alpha=0.5, figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ref: https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use k-folds cross-validation (k=3) to assess the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_column_name = list(X.columns)\n",
    "X_one_deg = pd.DataFrame(X[X_column_name])\n",
    "y = pd.DataFrame(df.target)\n",
    "model = LinearRegression()\n",
    "scores = []\n",
    "print(\"Coefficient of Determination (R2):\")\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for i, (train, test) in enumerate(kfold.split(X_one_deg, y)):\n",
    "    model.fit(X_one_deg.iloc[train,:], y.iloc[train,:])\n",
    "    score = model.score(X_one_deg.iloc[test,:], y.iloc[test,:])\n",
    "    scores.append(score)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ref: https://becominghuman.ai/linear-regression-in-python-with-pandas-scikit-learn-72574a2ec1a5 >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having already observed a nonlinear relationship between bike availability and the time of the day, we will now investigate whether our linear model would more effectively predict our target value if implemented polynomially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using scikit's built-in Polynomial Features\n",
    "polynomial_features= PolynomialFeatures(degree=2)\n",
    "X_poly = polynomial_features.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "y_poly_pred = model.predict(X_poly)\n",
    "df['deg_2_pred'] = y_poly_pred\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y,y_poly_pred))\n",
    "r2 = r2_score(y,y_poly_pred)\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "print(\"Coefficient of Determination (R2): \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[['target', 'deg_2_pred']].plot(alpha=0.5, figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model trained on polynomial data appears to offer a marginally improved r^2 value in comparison to the single dimensional based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Testing evaluation will be carried out on each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with train/test split - single dimensional & polynomial based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# Take a third (random) data samples as test data, rest as training data\n",
    "X_train_one_d, X_test_one_d, y_train_one_d, y_test_one_d = train_test_split(X, y, test_size=0.3)\n",
    "X_train_polynomial, X_test_polynomial, y_train_polynomial, y_test_polynomial = train_test_split(X_poly, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the training sample and test on the test sample.\n",
    "one_d_multi_linreg = LinearRegression().fit(X_train_one_d, y_train_one_d)\n",
    "polynomial_multi_linreg = LinearRegression().fit(X_train_polynomial, y_train_polynomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicted avaialbleBikes/availableBikeStands on training set\n",
    "one_d_train_predictions = one_d_multi_linreg.predict(X_train_one_d)\n",
    "polynomial_train_predictions = polynomial_multi_linreg.predict(X_train_polynomial)\n",
    "print(\"Training metrics for single dimensional model:\")\n",
    "printMetrics(y_train_one_d, one_d_train_predictions)\n",
    "print(\" \")\n",
    "print(\"Training metrics for polynomial model:\")\n",
    "printMetrics(y_train_polynomial, polynomial_train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training metrics for the polynomial model are marinally better than the single-dimensional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted avaialbleBikes/availableBikeStands on test set\n",
    "one_d_test_predictions = one_d_multi_linreg.predict(X_test_one_d)\n",
    "polynomial_test_predictions = polynomial_multi_linreg.predict(X_test_polynomial)\n",
    "\n",
    "# print(\"Actual values of one dimensionaltest:\\n\", y_test_one_d)\n",
    "# print(\"Predictions on single dimensional test:\", one_d_test_predictions)\n",
    "print(\"Testing metrics for single dimensional model:\")\n",
    "printMetrics(y_test_one_d, one_d_test_predictions)\n",
    "print(\" \")\n",
    "# print(\"Actual values of polynomial test:\\n\", y_test_polynomial)\n",
    "# print(\"Predictions on polynomial test:\", polynomial_test_predictions)\n",
    "print(\"Testing metrics for polynomial model:\")\n",
    "printMetrics(y_test_polynomial, polynomial_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can observe differences in the results produced by each model when trained on data from the days of the the week as opposed to the days of the weekend:\n",
    "#### The polynomial model, when introduced to unseen data, performs extremely poorly on the days of the week based data. This is likely due to overfitting. For this reason we will discard the polynomial model for our data based upon the days of the week.\n",
    "#### However, the polynomial model performs noticeably better than the single dimensional model when trained with data from the weekend days. For this reason we will use the polynomial model for the days of the weekend based model.\n",
    "## Cross validation of each model will now be performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with cross-validation - single dimensional based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_d_scores = -cross_val_score(LinearRegression(), X, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "one_d_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_d_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'r2']\n",
    "one_d_scores = cross_validate(LinearRegression(), X, y, scoring=one_d_metrics, cv=5)\n",
    "print(\"Metrics on cross validated one dimensional based model:\")\n",
    "one_d_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with cross-validation - polynomial based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_scores = -cross_val_score(LinearRegression(), X_poly, y, scoring='neg_mean_absolute_error', cv=5)\n",
    "polynomial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'r2']\n",
    "polynomial_scores = cross_validate(LinearRegression(), X_poly, y, scoring=polynomial_metrics, cv=5)\n",
    "print(\"Metrics on cross validated polynomial based model:\")\n",
    "polynomial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(scores.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
